{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efaf640",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0a904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_DIR = \"/home/bill/Desktop/kr/\"\n",
    "CLASSIFIER_NAME = \"klue/bert-base\"\n",
    "CLASSIFIER_DIR = KR_DIR + \"kr-honorifics-classification/trained_model/\"\n",
    "MODEL_NAME = \"gogamza/kobart-summarization\"\n",
    "MODEL_MAX_LENGTH = 128\n",
    "MODEL_OUTPUT_DIR = \"trained_model/\"\n",
    "MODEL_EVALUATION_STRATEGY = \"epoch\"\n",
    "MODEL_LEARNING_RATE = 2e-5\n",
    "MODEL_BATCH_SIZE = 128\n",
    "MODEL_WEIGHT_DECAY = 0.01\n",
    "MODEL_EPOCHS_NUM = 1\n",
    "MODEL_PREDICT_WITH_GENERATE = True\n",
    "MODEL_FP16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bb86bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc44f7",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82174f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-475d303c6f95de56\n",
      "Found cached dataset csv (/home/bill/.cache/huggingface/datasets/csv/default-475d303c6f95de56/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8a983d99f14c63a2cb1cc3fe706776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['non-honorific', 'honorific'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['non-honorific', 'honorific'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['non-honorific', 'honorific'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"eval\": \"eval.csv\", \"test\": \"test.csv\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee021d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non-honorific</th>\n",
       "      <th>honorific</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì €ê¸° ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì´ë‹ˆ ì—˜ì§€ë‹ˆ</td>\n",
       "      <td>ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì¸ì§€ ì—˜ì§€ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•¼ ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ ì—˜ì§€ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ</td>\n",
       "      <td>ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš” ì—˜ì§€ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì•¼ ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì´ë‘ ì—˜ì§€ë…¸íŠ¸ë¶ ì¤‘ì— ì–´ë–¤ ë…¸íŠ¸ë¶ ë°˜í’ˆí•˜ëŠ”ê±°ëƒ</td>\n",
       "      <td>ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì´ë‘ ì—˜ì§€ë…¸íŠ¸ë¶ ì¤‘ì— ì–´ë–¤ ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí•˜ëŠ”ê±°ì—ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì €ê¸° ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì´ë‘ ì—˜ì§€ ì¤‘ì— ë­”ì§€ ì•Œê³ ìˆë‹ˆ</td>\n",
       "      <td>í˜¹ì‹œ ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ê³¼ ì—˜ì§€ ì¤‘ì— ë¬´ì—‡ì¸ì§€ ì•„ì‹œë‚˜ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì €ê¸° ë‚´ì¼ ë…¸íŠ¸ë¶ ë°˜í’ˆí•˜ëŠ”ê²Œ ì‚¼ì„±ê±´ì§€ ì—˜ì§€ê±´ì§€ í™•ì¸ì¢€ í•´ë´</td>\n",
       "      <td>ë‚´ì¼ ë…¸íŠ¸ë¶ ë°˜í’ˆì´ ì‚¼ì„±ë…¸íŠ¸ë¶ì¸ì§€ ì—˜ì§€ë…¸íŠ¸ë¶ì¸ì§€ í™•ì¸ë¶€íƒë“œë¦½ë‹ˆë‹¤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         non-honorific                            honorific\n",
       "0              ì €ê¸° ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì´ë‹ˆ ì—˜ì§€ë‹ˆ          ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì¸ì§€ ì—˜ì§€ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”\n",
       "1         ì•¼ ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ ì—˜ì§€ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ       ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš” ì—˜ì§€ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš”\n",
       "2  ì•¼ ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì´ë‘ ì—˜ì§€ë…¸íŠ¸ë¶ ì¤‘ì— ì–´ë–¤ ë…¸íŠ¸ë¶ ë°˜í’ˆí•˜ëŠ”ê±°ëƒ  ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì´ë‘ ì—˜ì§€ë…¸íŠ¸ë¶ ì¤‘ì— ì–´ë–¤ ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí•˜ëŠ”ê±°ì—ìš”\n",
       "3    ì €ê¸° ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ì´ë‘ ì—˜ì§€ ì¤‘ì— ë­”ì§€ ì•Œê³ ìˆë‹ˆ   í˜¹ì‹œ ë‚´ì¼ ë°˜í’ˆí•  ë…¸íŠ¸ë¶ì´ ì‚¼ì„±ê³¼ ì—˜ì§€ ì¤‘ì— ë¬´ì—‡ì¸ì§€ ì•„ì‹œë‚˜ìš”\n",
       "4     ì €ê¸° ë‚´ì¼ ë…¸íŠ¸ë¶ ë°˜í’ˆí•˜ëŠ”ê²Œ ì‚¼ì„±ê±´ì§€ ì—˜ì§€ê±´ì§€ í™•ì¸ì¢€ í•´ë´  ë‚´ì¼ ë…¸íŠ¸ë¶ ë°˜í’ˆì´ ì‚¼ì„±ë…¸íŠ¸ë¶ì¸ì§€ ì—˜ì§€ë…¸íŠ¸ë¶ì¸ì§€ í™•ì¸ë¶€íƒë“œë¦½ë‹ˆë‹¤"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7cc40",
   "metadata": {},
   "source": [
    "# Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828750a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a3d0a6286c4258b1890c9803b8b9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6487c0d3e04629a270c7aa78f3bbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e690d76b73437c96b4405f1dd6a239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/177k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5927cc3da946b1acc2a0f9c79f4a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07860753d554463fad2697e25da4bd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed22d9dbc9f1468bb6b0fa9ddb072d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9517, 12041, 14588, 25569, 10949, 12034, 10227, 16584, 24828, 13328, 10949, 17003, 14593, 27476, 12007, 14149, 13514, 14049, 9031, 11786, 11900], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer(\"ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì´ë‘ ì—˜ì§€ë…¸íŠ¸ë¶ ì¤‘ì— ì–´ë–¤ ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí•˜ëŠ”ê±°ì—ìš”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1073ce",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023c3920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8909086d153493682431342301e1e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/miniconda3/envs/krenv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e469656a30402392e9cc6fb159c100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ec37bfe9d440bea98e72930f9f4c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['non-honorific', 'honorific', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['non-honorific', 'honorific', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['non-honorific', 'honorific', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(dataslice):\n",
    "  model_inputs = tokenizer(dataslice[\"non-honorific\"], truncation=True, max_length=MODEL_MAX_LENGTH)\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(dataslice[\"honorific\"], truncation=True, max_length=MODEL_MAX_LENGTH)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "processed_dataset = dataset.map(process, batched=True)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366c0315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non-honorific': 'ì•¼ ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ ì—˜ì§€ë…¸íŠ¸ë¶ ë°˜í’ˆí• ê±°ëƒ',\n",
       " 'honorific': 'ë‚´ì¼ ì‚¼ì„±ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš” ì—˜ì§€ë…¸íŠ¸ë¶ì„ ë°˜í’ˆí• ê±°ì—ìš”',\n",
       " 'input_ids': [11734,\n",
       "  18683,\n",
       "  14588,\n",
       "  25569,\n",
       "  10949,\n",
       "  14149,\n",
       "  13514,\n",
       "  13594,\n",
       "  9031,\n",
       "  9531,\n",
       "  16584,\n",
       "  24828,\n",
       "  13328,\n",
       "  10949,\n",
       "  14149,\n",
       "  13514,\n",
       "  13594,\n",
       "  9031,\n",
       "  9531],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [9517,\n",
       "  12041,\n",
       "  14588,\n",
       "  25569,\n",
       "  10949,\n",
       "  12007,\n",
       "  14149,\n",
       "  13514,\n",
       "  13594,\n",
       "  9031,\n",
       "  11786,\n",
       "  11900,\n",
       "  16584,\n",
       "  24828,\n",
       "  13328,\n",
       "  10949,\n",
       "  12007,\n",
       "  14149,\n",
       "  13514,\n",
       "  13594,\n",
       "  9031,\n",
       "  11786,\n",
       "  11900]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5a80f",
   "metadata": {},
   "source": [
    "# Set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d0db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f42a77dec2549869f898929f9701419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2626375",
   "metadata": {},
   "source": [
    "# Set trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e45617",
   "metadata": {},
   "source": [
    "## Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7137513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=MODEL_OUTPUT_DIR,\n",
    "  evaluation_strategy=MODEL_EVALUATION_STRATEGY,\n",
    "  learning_rate=MODEL_LEARNING_RATE,\n",
    "  per_device_train_batch_size=MODEL_BATCH_SIZE,\n",
    "  per_device_eval_batch_size=MODEL_BATCH_SIZE,\n",
    "  weight_decay=MODEL_WEIGHT_DECAY,\n",
    "  num_train_epochs=MODEL_EPOCHS_NUM,\n",
    "  predict_with_generate=MODEL_PREDICT_WITH_GENERATE,\n",
    "  fp16=MODEL_FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20d14b",
   "metadata": {},
   "source": [
    "## Set data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ef41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizerFast(name_or_path='gogamza/kobart-summarization', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}), model=BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65ca7b",
   "metadata": {},
   "source": [
    "## Set metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7fbbd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10283/3377329550.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3f7964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "  predictions = [prediction.strip() for prediction in predictions]\n",
    "  labels = [[label.strip()] for label in labels]\n",
    "  return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a833624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(evaluation_predictions):\n",
    "  predictions, labels = evaluation_predictions\n",
    "  if isinstance(predictions, tuple):\n",
    "    predictions = predictions[0]\n",
    "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  decoded_predictions, decoded_labels = postprocess(decoded_predictions, decoded_labels)\n",
    "  result = metric.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "  result = {\"bleu\": result[\"score\"]}\n",
    "  prediction_lengths = [np.count_nonzero(prediction != tokenizer.pad_token_id) for prediction in predictions]\n",
    "  result[\"get_len\"] = np.mean(prediction_lengths)\n",
    "  result = {key: round(value, 4) for key, value in result.items()}\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50f8bc",
   "metadata": {},
   "source": [
    "## Set trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b8863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=processed_dataset[\"train\"],\n",
    "  eval_dataset=processed_dataset[\"eval\"],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc21d6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931d8ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: non-honorific, honorific. If non-honorific, honorific are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/bill/miniconda3/envs/krenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 94\n",
      "  Number of trainable parameters = 123859968\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Get Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.098305</td>\n",
       "      <td>32.057000</td>\n",
       "      <td>19.870000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: non-honorific, honorific. If non-honorific, honorific are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 128\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "if train_model:\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0fe884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/config.json\n",
      "Model weights saved in trained_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "if save_model:\n",
    "  model.save_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8221e4f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41589d6a",
   "metadata": {},
   "source": [
    "## Generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85f583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file trained_model/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-summarization\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_model = BartForConditionalGeneration.from_pretrained(MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4156e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ì‚¼ì„±ì´ë‘ ì—˜ì§€ ì¤‘ì— ì—ì–´ì»¨ ë” ì‹¼ ê³³ í™•ì¸í•´ì¤„ë˜',\n",
       " 'ì—ì–´ì»¨ì´ ë” ì‹¼ ê³³ì´ ì‚¼ì„±ì´ëƒ ì—˜ì§€ëƒ',\n",
       " 'ì‚¼ì„± ì—ì–´ì»¨ ì´ ë” ì‹¸ëƒ ì—˜ì§€ê°€ ë” ì‹¸ëƒ',\n",
       " 'ì–˜ ì—ì–´ì»¨ì´ ë” ì‹¼ ê³³ì´ ì‚¼ì„±ì´ë‹ˆ ì—˜ì§€ë‹ˆ',\n",
       " 'ì €ê¸° ì‚¼ì„±ê³¼ ì—˜ì§€ ì¤‘ì— ì—ì–´ì»¨ ë” ì‹¼ ê³³ ì•Œê³ ìˆë‹ˆ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = processed_dataset[\"test\"][\"non-honorific\"]\n",
    "test_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e069319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11220,\n",
       " 14439,\n",
       " 10227,\n",
       " 16584,\n",
       " 12332,\n",
       " 17003,\n",
       " 26232,\n",
       " 14166,\n",
       " 24022,\n",
       " 14551,\n",
       " 14715,\n",
       " 25040,\n",
       " 10232,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer(test_texts, padding=True, truncation=True, max_length=MODEL_MAX_LENGTH)\n",
    "model_inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b80173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 11220,\n",
       " 14439,\n",
       " 10227,\n",
       " 16584,\n",
       " 12332,\n",
       " 17003,\n",
       " 26232,\n",
       " 14166,\n",
       " 24022,\n",
       " 14551,\n",
       " 14715,\n",
       " 25040,\n",
       " 10232,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs[\"input_ids\"] = [[tokenizer.bos_token_id] + input_indices + [tokenizer.eos_token_id]\n",
    "                             for input_indices in model_inputs[\"input_ids\"]]\n",
    "model_inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58d4755e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/miniconda3/envs/krenv/lib/python3.8/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    2, 11220, 15355, 11863, 16584, 12332, 17003, 26232, 14166, 24022,\n",
       "        19105, 14715, 17801, 17275,     1,     3,     3,     3,     3,     3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs = trained_model.generate(torch.tensor(model_inputs[\"input_ids\"]))\n",
    "model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de51e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ì‚¼ì„±ê³¼ì™€ ì—˜ì§€ ì¤‘ì— ì—ì–´ì»¨ ë” ì‹¼ ê³³ì€ í™•ì¸í•´ì£¼ì„¸ìš”',\n",
       " 'ì—ì–´ì»¨ì´ ë” ì‹¼ ê³³ì´ ì‚¼ì„±ì´ëƒ ì—˜ì§€ëƒì— ëŒ€í•´ ì‚¼ì„±ê³¼ ì—˜ì§€ê°€',\n",
       " 'ì‚¼ì„± ì—ì–´ì»¨ ì—ì–´ì»¨ ì´ ë” ì‹¸ëƒ ì—˜ì§€ê°€ ë” ì‹¸ëƒìš” ì‚¼ì„± ì—ì–´ì»¨ ì´ ë” ì‹¸',\n",
       " 'ì—ì–´ì»¨ì´ ë” ì‹¼ ê³³ì€ ì‚¼ì„±ì´ë‹ˆ ì—˜ì§€ë‹ˆ í•˜ëŠ” ë§ì´ ìˆì£  ì—ì–´ì»¨ì´ ë” ì‹¼',\n",
       " 'ì‚¼ì„±ê³¼ ì—˜ì§€ ì¤‘ì— ì—ì–´ì»¨ ë” ì‹¼ ê³³ì€ ì•Œê³ ìˆë‚˜ìš” ì‚¼ì„±ê³¼ ì—˜ì§€ ì¤‘ì—']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(model_outputs, skip_special_tokens=True)\n",
    "outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c1223",
   "metadata": {},
   "source": [
    "## Load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e90e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/bill/.cache/huggingface/hub/models--klue--bert-base/snapshots/812449f1a6bc736e693db7aa0e513e5e90795a62/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file /home/bill/Desktop/kr/kr-honorifics-classification/trained_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /home/bill/Desktop/kr/kr-honorifics-classification/trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/bill/Desktop/kr/kr-honorifics-classification/trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained(CLASSIFIER_NAME)\n",
    "classifier = BertForSequenceClassification.from_pretrained(CLASSIFIER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93e83a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f874bb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.4841,  7.5277],\n",
       "        [ 6.1777, -6.3077],\n",
       "        [-1.3936,  1.3218],\n",
       "        [-7.2436,  7.2471],\n",
       "        [-7.3923,  7.4635]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_inputs = classifier_tokenizer(outputs, padding=True, return_tensors=\"pt\")\n",
    "logits = classifier(**classifier_inputs).logits\n",
    "logits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09462485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0231e-07, 1.0000e+00],\n",
       "        [1.0000e+00, 3.7816e-06],\n",
       "        [6.2070e-02, 9.3793e-01],\n",
       "        [5.0902e-07, 1.0000e+00],\n",
       "        [3.5335e-07, 1.0000e+00]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predicted_probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "predicted_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6904b427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = np.argmax(predicted_probabilities.detach().numpy(), axis=1) # TODO add categories\n",
    "predicted_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f10293a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7408333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honorifics_num = 0\n",
    "for predicted_label in predicted_labels:\n",
    "  if predicted_label == 1:\n",
    "    honorifics_num += 1\n",
    "honorifics_num / len(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
